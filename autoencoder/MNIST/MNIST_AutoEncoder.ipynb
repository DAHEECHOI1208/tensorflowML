{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 180, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_task_type': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1255dcfd0>, '_model_dir': '/tmp/trained_model', '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_session_config': None, '_tf_random_seed': None, '_save_summary_steps': 100, '_environment': 'local', '_num_worker_replicas': 0, '_task_id': 0, '_log_step_count_steps': 100, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", '_evaluation_master': '', '_master': ''}\n",
      "WARNING:tensorflow:From /Users/terrycho/anaconda/envs/tensorflow1.0/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/monitors.py:269: __init__ (from tensorflow.contrib.learn.python.learn.monitors) is deprecated and will be removed after 2016-12-05.\n",
      "Instructions for updating:\n",
      "Monitors are deprecated. Please use tf.train.SessionRunHook.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/trained_model/model.ckpt.\n",
      "INFO:tensorflow:Starting evaluation at 2017-09-13-10:27:01\n",
      "INFO:tensorflow:Restoring parameters from /tmp/trained_model/model.ckpt-1\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2017-09-13-10:27:02\n",
      "INFO:tensorflow:Saving dict for global step 1: global_step = 1, loss = 18103.0\n",
      "INFO:tensorflow:Validation (step 1): loss = 18103.0, global_step = 1\n",
      "INFO:tensorflow:loss = 18179.7, step = 1\n",
      "INFO:tensorflow:global_step/sec: 36.1027\n",
      "INFO:tensorflow:loss = 4708.8, step = 101 (1.596 sec)\n",
      "INFO:tensorflow:global_step/sec: 57.3873\n",
      "INFO:tensorflow:loss = 3521.97, step = 201 (1.743 sec)\n",
      "INFO:tensorflow:global_step/sec: 56.1699\n",
      "INFO:tensorflow:loss = 3137.55, step = 301 (1.781 sec)\n",
      "INFO:tensorflow:global_step/sec: 55.329\n",
      "INFO:tensorflow:loss = 2415.34, step = 401 (1.807 sec)\n",
      "INFO:tensorflow:global_step/sec: 58.606\n",
      "INFO:tensorflow:loss = 2082.85, step = 501 (1.706 sec)\n",
      "INFO:tensorflow:global_step/sec: 57.2172\n",
      "INFO:tensorflow:loss = 1989.09, step = 601 (1.747 sec)\n",
      "INFO:tensorflow:global_step/sec: 57.9052\n",
      "INFO:tensorflow:loss = 1779.19, step = 701 (1.728 sec)\n",
      "INFO:tensorflow:global_step/sec: 59.1852\n",
      "INFO:tensorflow:loss = 1797.36, step = 801 (1.689 sec)\n",
      "INFO:tensorflow:global_step/sec: 53.6741\n",
      "INFO:tensorflow:loss = 1915.36, step = 901 (1.864 sec)\n",
      "INFO:tensorflow:global_step/sec: 56.5979\n",
      "INFO:tensorflow:loss = 1651.84, step = 1001 (1.766 sec)\n",
      "INFO:tensorflow:global_step/sec: 50.6036\n",
      "INFO:tensorflow:loss = 1556.03, step = 1101 (1.976 sec)\n",
      "INFO:tensorflow:global_step/sec: 51.909\n",
      "INFO:tensorflow:loss = 1491.89, step = 1201 (1.928 sec)\n",
      "INFO:tensorflow:global_step/sec: 46.0575\n",
      "INFO:tensorflow:loss = 1474.74, step = 1301 (2.170 sec)\n",
      "INFO:tensorflow:global_step/sec: 51.1926\n",
      "INFO:tensorflow:loss = 1466.62, step = 1401 (1.953 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.8465\n",
      "INFO:tensorflow:loss = 1614.0, step = 1501 (2.090 sec)\n",
      "INFO:tensorflow:global_step/sec: 46.3934\n",
      "INFO:tensorflow:loss = 1512.07, step = 1601 (2.156 sec)\n",
      "INFO:tensorflow:global_step/sec: 50.3385\n",
      "INFO:tensorflow:loss = 1357.44, step = 1701 (1.987 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.2793\n",
      "INFO:tensorflow:loss = 1324.15, step = 1801 (2.115 sec)\n",
      "INFO:tensorflow:global_step/sec: 45.8234\n",
      "INFO:tensorflow:loss = 1249.29, step = 1901 (2.183 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2000 into /tmp/trained_model/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 1601.86.\n",
      "INFO:tensorflow:Starting evaluation at 2017-09-13-10:27:42\n",
      "INFO:tensorflow:Restoring parameters from /tmp/trained_model/model.ckpt-2000\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2017-09-13-10:27:42\n",
      "INFO:tensorflow:Saving dict for global step 2000: global_step = 2000, loss = 1332.06\n",
      "INFO:tensorflow:Restoring parameters from /tmp/trained_model/model.ckpt-2000\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: /tmp/trained_model/export/Servo/1505298462/saved_model.pb\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'global_step': 2000, 'loss': 1332.0552},\n",
       " ['/tmp/trained_model/export/Servo/1505298462'])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#original code https://github.com/enakai00/autoencoder_example\n",
    "\n",
    "import tensorflow as tf\n",
    "import os,shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.python.estimator.model_fn import ModeKeys as Modes\n",
    "from tensorflow.contrib.learn import Experiment\n",
    "from tensorflow.contrib.learn.python.learn import learn_runner\n",
    "from tensorflow.contrib.learn.python.learn.utils import (saved_model_export_utils)\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "OUTDIR='/tmp/trained_model'\n",
    "\n",
    "#\n",
    "# definining queue\n",
    "#\n",
    "def read_and_decode(filename_queue):\n",
    "    reader = tf.TFRecordReader()\n",
    "    _,serialized_example = reader.read(filename_queue)\n",
    "    \n",
    "    features = tf.parse_single_example(\n",
    "        serialized_example,\n",
    "        features={\n",
    "            'image_raw':tf.FixedLenFeature([],tf.string),\n",
    "            'label':tf.FixedLenFeature([],tf.int64),\n",
    "        })\n",
    "    \n",
    "    image = tf.decode_raw(features['image_raw'],tf.uint8)\n",
    "    image.set_shape([784]) #image shape is (784,)\n",
    "    image = tf.cast(image,tf.float32)*(1.0/255)\n",
    "    label = tf.cast(features['label'],tf.int32)\n",
    "    \n",
    "    return image,label\n",
    "\n",
    "def input_fn(filename,batch_size=100):\n",
    "    filename_queue = tf.train.string_input_producer([filename])\n",
    "    \n",
    "    image,label = read_and_decode(filename_queue)\n",
    "    images,labels = tf.train.batch(\n",
    "        [image,label],batch_size=batch_size,\n",
    "        capacity=1000+3*batch_size)\n",
    "    #images : (100,784), labels : (100,1)\n",
    "    \n",
    "    return {'inputs':images},labels\n",
    "\n",
    "def get_input_fn(filename,batch_size=100):\n",
    "    return lambda: input_fn(filename,batch_size)\n",
    "\n",
    "def serving_input_fn():\n",
    "    inputs = {'inputs':tf.placeholder(tf.float32,[None,784])}\n",
    "    return tf.estimator.export.ServingInputReceiver(inputs,inputs)\n",
    "\n",
    "#\n",
    "# define model\n",
    "#\n",
    "def autoencoder_model_fn(features,labels,mode):\n",
    "    input_layer = features['inputs']\n",
    "    dense1 = tf.layers.dense(inputs=input_layer,units=256,activation=tf.nn.relu)\n",
    "    dense2 = tf.layers.dense(inputs=dense1,units=128,activation=tf.nn.relu)\n",
    "    dense3 = tf.layers.dense(inputs=dense2,units=16,activation=tf.nn.relu)\n",
    "    dense4 = tf.layers.dense(inputs=dense3,units=128,activation=tf.nn.relu)\n",
    "    dense5 = tf.layers.dense(inputs=dense4,units=256,activation=tf.nn.relu)\n",
    "    output_layer = tf.layers.dense(inputs=dense5,units=784,activation=tf.nn.sigmoid)\n",
    "    \n",
    "    #training and evaluation mode\n",
    "    if mode in (Modes.TRAIN,Modes.EVAL):\n",
    "        global_step = tf.contrib.framework.get_or_create_global_step()\n",
    "        label_indices = tf.cast(labels,tf.int32)\n",
    "        loss = tf.reduce_sum(tf.square(output_layer - input_layer))\n",
    "        tf.summary.scalar('OptimizeLoss',loss)\n",
    "\n",
    "        if mode == Modes.TRAIN:\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "            train_op = optimizer.minimize(loss,global_step=global_step)\n",
    "            return tf.estimator.EstimatorSpec(mode,loss = loss, train_op = train_op)\n",
    "        if mode == Modes.EVAL:\n",
    "            eval_metric_ops = None\n",
    "            return tf.estimator.EstimatorSpec(\n",
    "                mode,loss=loss,eval_metric_ops = eval_metric_ops)\n",
    "        \n",
    "    # prediction mode\n",
    "    if mode == Modes.PREDICT:\n",
    "        predictions={\n",
    "            'outputs':output_layer\n",
    "        }\n",
    "        export_outputs={\n",
    "            'outputs':tf.estimator.export.PredictOutput(predictions)\n",
    "        }\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode,predictions=predictions,export_outputs=export_outputs) #이부분 코드 상세 조사할것\n",
    "    \n",
    "\n",
    "def build_estimator(model_dir):\n",
    "    return tf.estimator.Estimator(\n",
    "        model_fn = autoencoder_model_fn,\n",
    "        model_dir = model_dir,\n",
    "        config=tf.contrib.learn.RunConfig(save_checkpoints_secs=180))\n",
    "\n",
    "def generate_experiment_fn(data_dir,\n",
    "                          train_batch_size = 100,\n",
    "                          eval_batch_size = 100,\n",
    "                          train_steps = 1000,\n",
    "                          eval_steps = 1,\n",
    "                          **experiment_args):\n",
    "    def _experiment_fn(output_dir):\n",
    "        return Experiment(\n",
    "            build_estimator(output_dir),\n",
    "            train_input_fn=get_input_fn('./data/train.tfrecords',batch_size=train_batch_size),\n",
    "            eval_input_fn=get_input_fn('./data/test.tfrecords',batch_size=eval_batch_size),\n",
    "            export_strategies = [saved_model_export_utils.make_export_strategy(\n",
    "                serving_input_fn,\n",
    "                default_output_alternative_key=None,\n",
    "                exports_to_keep=1)\n",
    "            ],\n",
    "            train_steps = train_steps,\n",
    "            eval_steps = eval_steps,\n",
    "            **experiment_args\n",
    "        )\n",
    "    return _experiment_fn\n",
    "\n",
    "shutil.rmtree(OUTDIR, ignore_errors=True) # start fresh each time\n",
    "learn_runner.run(\n",
    "    generate_experiment_fn(\n",
    "        data_dir='./data/',\n",
    "        train_steps=2000),\n",
    "    OUTDIR)\n",
    "            \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def printx():\n",
    "    print('hello')\n",
    "    \n",
    "f = lambda: printx()\n",
    "\n",
    "print(f())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
